----------- a ----------
Batch gradient descent is impemented

Learning rate used = 0.1

stopping criteria : the algorithm stops whenever the change in the cost is less
than a particular epsilon.
value of epsilon chosen = 0.00001

Final parameters obtained are as follows:

theta0 = 5.8329
theta1 = 4.6116

cost = 4.4770

----------- b ----------
the line obtained by the algorithm is plotted

----------- c ----------
3d mesh showing the error function is plotted. Error at current iteration is shown
and the camera view point height is adjusted at each iteration to show the error
at current iteration. (The idea of adjusting the height of camera is taken from
an answer on piazza).

----------- d and e ----------
Contours are plotted  at each iteration. 

Learning rate = 0.01
epsilon = 0.00001

It is observed that the algorithm converges for learning rate = 0.1, 0.5, 0.9 and 1.3 and
converges faster as the learning rate is increased from 0.1 to 1.3

It is further observed that the algorithm diverges at learning rate = 2.1 and 2.5
It happens because it moves too much in the given direction and oscillates while
climbing up the mesh of the error function


